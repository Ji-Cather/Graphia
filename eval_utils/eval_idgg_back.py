import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from sklearn.preprocessing import MinMaxScaler

def normalize_metrics_for_idgg(df):
    """
    å¯¹ IDGG æŒ‡æ ‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼ˆæŒ‰æ•°æ®é›†åˆ†åˆ«å½’ä¸€åŒ–ï¼‰
    è´Ÿå‘æŒ‡æ ‡éœ€è¦åå‘å¤„ç†
    """
    # å¤åˆ¶æ•°æ®æ¡†ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    normalized_df = df.copy()
    
    # è·å–æ‰€æœ‰è´Ÿå‘æŒ‡æ ‡åˆ— (éœ€è¦åå‘)
    negative_metrics = [col for col in df.columns if col in [
        'graph_list_degree_mmd', 'graph_list_cluster_mmd', 'graph_list_spectra_mmd',
        'graph_macro_D', 'graph_macro_num_chambers_diff'
    ]]
    
    # è·å–æ‰€æœ‰æ­£å‘æŒ‡æ ‡åˆ—
    positive_metrics = [col for col in df.columns if col in [
        'graph_edge_overlap', 'graph_macro_auc@100_hub'
    ]]
    
    # print(f"Negative metrics (to be reversed): {negative_metrics}")
    # print(f"Positive metrics: {positive_metrics}")
    
    # æŒ‰æ•°æ®é›†åˆ†åˆ«è¿›è¡Œå½’ä¸€åŒ–
    for dataset in df['dataset'].unique():
        print(f"\nProcessing dataset: {dataset}")
        # è·å–å½“å‰æ•°æ®é›†çš„æ•°æ®ç´¢å¼•
        dataset_mask = df['dataset'] == dataset
        dataset_indices = df[dataset_mask].index
        
        # æ˜¾ç¤ºåŸå§‹æ•°æ®
        # print("Original data for this dataset:")
        metric_columns = [col for col in df.columns if col.startswith('graph_')]
        # print(df.loc[dataset_indices, ['model'] + metric_columns])
        
        # å¯¹è´Ÿå‘æŒ‡æ ‡è¿›è¡Œå½’ä¸€åŒ–å¹¶åå‘ (1 - x) (è¿™äº›æŒ‡æ ‡è¶Šä½è¶Šå¥½)
        if negative_metrics:
            scaler = MinMaxScaler()
            original_values = df.loc[dataset_indices, negative_metrics].fillna(0)
            normalized_values = scaler.fit_transform(original_values)
            # print("Normalized negative values before reversing:")
            # print(normalized_values)
            # åå‘å¤„ç†è´Ÿå‘æŒ‡æ ‡
            reversed_values = 1 - normalized_values
            normalized_df.loc[dataset_indices, negative_metrics] = reversed_values
            # print("Reversed negative values (final):")
            # print(reversed_values)
        
        # å¯¹æ­£å‘æŒ‡æ ‡è¿›è¡Œå½’ä¸€åŒ– (è¿™äº›æŒ‡æ ‡è¶Šé«˜è¶Šå¥½)
        if positive_metrics:
            scaler = MinMaxScaler()
            original_values = df.loc[dataset_indices, positive_metrics].fillna(0)
            normalized_values = scaler.fit_transform(original_values)
            normalized_df.loc[dataset_indices, positive_metrics] = normalized_values
            print("Normalized positive metrics:")
            print(normalized_values)
    
    return normalized_df

def calculate_idgg_social_fidelity_scores(original_df, normalized_df, weights=None):
    """
    è®¡ç®— IDGG social fidelity scores
    åŒ…æ‹¬ä¸‰ä¸ªå­åˆ†æ•°:
    1. macro_structure_score: å®è§‚æ‹ŸçœŸæ‹“æ‰‘ç»“æ„æŒ‡æ ‡ (degree_mmd, cluster_mmd, spectra_mmd, edge_overlap)
    2. macro_phenomenon_score: å®è§‚ç°è±¡æ‹ŸåˆæŒ‡æ ‡ (D, auc@100_hub, num_chambers_diff)
    3. idgg_social_fidelity_score: ç»¼åˆåˆ†æ•°
    """
    if weights is None:
        # é»˜è®¤æƒé‡
        weights = {
            'macro_structure': 0.4,
            'macro_phenomenon': 0.6
        }
    
    # åˆ›å»ºç»“æœDataFrameï¼ŒåŒ…å«åŸå§‹æ•°æ®
    result_df = original_df.copy()
    
    # å®è§‚æ‹ŸçœŸæ‹“æ‰‘ç»“æ„æŒ‡æ ‡ (è´Ÿå‘æŒ‡æ ‡ï¼Œå·²åå‘å¤„ç†)
    macro_structure_metrics = [col for col in normalized_df.columns if col in [
        'graph_list_degree_mmd', 'graph_list_cluster_mmd', 'graph_list_spectra_mmd',
        'graph_edge_overlap'
    ]]
    
    # å®è§‚ç°è±¡æ‹ŸåˆæŒ‡æ ‡ (è´Ÿå‘æŒ‡æ ‡ï¼Œå·²åå‘å¤„ç†)
    macro_phenomenon_metrics = [col for col in normalized_df.columns if col in [
        'graph_macro_D', 'graph_macro_auc@100_hub', 'graph_macro_num_chambers_diff'
    ]]
    
    print(f"Macro structure metrics: {macro_structure_metrics}")
    print(f"Macro phenomenon metrics: {macro_phenomenon_metrics}")
    
    # è®¡ç®—å®è§‚æ‹ŸçœŸæ‹“æ‰‘ç»“æ„å¾—åˆ†ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„å€¼ï¼‰
    if macro_structure_metrics:
        result_df['macro_structure_score'] = normalized_df[macro_structure_metrics].mean(axis=1)
    else:
        result_df['macro_structure_score'] = 0
    
    # è®¡ç®—å®è§‚ç°è±¡æ‹Ÿåˆå¾—åˆ†ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„å€¼ï¼‰
    if macro_phenomenon_metrics:
        result_df['macro_phenomenon_score'] = normalized_df[macro_phenomenon_metrics].mean(axis=1)
    else:
        result_df['macro_phenomenon_score'] = 0
    
    # è®¡ç®—æœ€ç»ˆçš„ idgg-social fidelity score
    result_df['idgg_social_fidelity_score'] = (
        weights['macro_structure'] * result_df['macro_structure_score'] + 
        weights['macro_phenomenon'] * result_df['macro_phenomenon_score']
    )
    
    return result_df

import re

def rename_retrieval_model(model_name):
    if re.match(r'grpo_.*_LIKR_reward_query_.*', model_name):
        return 'LLMGGen-seq'
    elif model_name.startswith('grpo_'):
        return 'LLMGGen'
    return model_name

def load_and_process_graph_list_data(file_path, exclude_models=None):
    """
    åŠ è½½å¹¶å¤„ç† graph list æ•°æ® (merged_graph_list_matrix.csv)
    å¤„ç†æŒ‡æ ‡: degree_mmd, cluster_mmd, spectra_mmd, D
    """
    df = pd.read_csv(file_path)
    
    # å¦‚æœæä¾›äº†è¦æ’é™¤çš„æ¨¡å‹åˆ—è¡¨ï¼Œåˆ™è¿‡æ»¤æ‰è¿™äº›æ¨¡å‹
    if exclude_models:
        df = df[~df['model'].isin(exclude_models)]
        print(f"ğŸ” ä» graph list æ•°æ®ä¸­æ’é™¤äº† {len(exclude_models)} ä¸ªæ¨¡å‹: {exclude_models}")
    
    # æ¨¡å‹é‡å‘½åé€»è¾‘
    # å¯¹äºæ»¡è¶³grpo_çš„éƒ½renameä¸ºLLMGGen
    df['model'] = df['model'].apply(rename_retrieval_model)
    
    # é€‰æ‹©éœ€è¦çš„æŒ‡æ ‡
    metrics = ['degree_mmd', 'cluster_mmd', 'spectra_mmd']
    
    # ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
    required_columns = ['model', 'dataset'] + metrics
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"ç¼ºå¤±åˆ—: {missing_columns}")
    
    # æŒ‰ modelã€dataset åˆ†ç»„å¹¶è®¡ç®—å¹³å‡å€¼
    grouped_df = df.groupby(['model', 'dataset'])[metrics].mean().reset_index()
    
    # é‡å‘½ååˆ—ä»¥æ ‡è¯†æ¥æº
    rename_dict = {metric: f"graph_list_{metric}" for metric in metrics}
    grouped_df.rename(columns=rename_dict, inplace=True)
    
    return grouped_df

def load_and_process_graph_data(file_path, exclude_models=None):
    """
    åŠ è½½å¹¶å¤„ç† graph æ•°æ® (merged_graph_matrix.csv)
    å¤„ç†æŒ‡æ ‡: wedge_count, triangle_count, edge_overlap
    """
    df = pd.read_csv(file_path)
    
    # å¦‚æœæä¾›äº†è¦æ’é™¤çš„æ¨¡å‹åˆ—è¡¨ï¼Œåˆ™è¿‡æ»¤æ‰è¿™äº›æ¨¡å‹
    if exclude_models:
        df = df[~df['model'].isin(exclude_models)]
        print(f"ğŸ” ä» graph æ•°æ®ä¸­æ’é™¤äº† {len(exclude_models)} ä¸ªæ¨¡å‹: {exclude_models}")
    
    # æ¨¡å‹é‡å‘½åé€»è¾‘
    # å¯¹äºæ»¡è¶³grpo_çš„éƒ½renameä¸ºLLMGGen
    df['model'] = df['model'].apply(rename_retrieval_model)
    
    # é€‰æ‹©éœ€è¦çš„æŒ‡æ ‡
    metrics = [ 'edge_overlap']
    
    # ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
    required_columns = ['model', 'dataset'] + metrics
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"ç¼ºå¤±åˆ—: {missing_columns}")
    
    # æŒ‰ modelã€dataset åˆ†ç»„å¹¶è®¡ç®—å¹³å‡å€¼
    grouped_df = df.groupby(['model', 'dataset'])[metrics].mean().reset_index()
    
    # é‡å‘½ååˆ—ä»¥æ ‡è¯†æ¥æº
    rename_dict = {metric: f"graph_{metric}" for metric in metrics}
    grouped_df.rename(columns=rename_dict, inplace=True)
    
    return grouped_df

def load_and_process_graph_macro_data(file_path, exclude_models=None):
    """
    åŠ è½½å¹¶å¤„ç† graph macro æ•°æ® (merged_graph_macro_matrix.csv)
    å¤„ç†æŒ‡æ ‡: num_chambers_diff, auc@100_hub, D
    """
    df = pd.read_csv(file_path)
    
    # å¦‚æœæä¾›äº†è¦æ’é™¤çš„æ¨¡å‹åˆ—è¡¨ï¼Œåˆ™è¿‡æ»¤æ‰è¿™äº›æ¨¡å‹
    if exclude_models:
        df = df[~df['model'].isin(exclude_models)]
        print(f"ğŸ” ä» graph macro æ•°æ®ä¸­æ’é™¤äº† {len(exclude_models)} ä¸ªæ¨¡å‹: {exclude_models}")
    
    # æ¨¡å‹é‡å‘½åé€»è¾‘
    # å¯¹äºæ»¡è¶³grpo_çš„éƒ½renameä¸ºLLMGGen
    df['model'] = df['model'].apply(rename_retrieval_model)
    
    # é€‰æ‹©éœ€è¦çš„æŒ‡æ ‡
    metrics = ['num_chambers_diff', 'auc@100_hub', 'D']
    
    # ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
    required_columns = ['model', 'dataset'] + metrics
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"ç¼ºå¤±åˆ—: {missing_columns}")
    
    # æŒ‰ modelã€dataset åˆ†ç»„å¹¶è®¡ç®—å¹³å‡å€¼
    grouped_df = df.groupby(['model', 'dataset'])[metrics].mean().reset_index()
    
    # é‡å‘½ååˆ—ä»¥æ ‡è¯†æ¥æº
    rename_dict = {metric: f"graph_macro_{metric}" for metric in metrics}
    grouped_df.rename(columns=rename_dict, inplace=True)
    
    return grouped_df

def merge_all_datasets(graph_list_df, graph_df, graph_macro_df):
    """
    åˆå¹¶æ‰€æœ‰æ•°æ®é›†ï¼Œåªä¿ç•™æ‰€æœ‰è¡¨éƒ½åŒ…å«çš„ modelã€dataset ç»„åˆ
    """
    # åˆå¹¶ä¸‰ä¸ªæ•°æ®æ¡†
    merged_df = pd.merge(graph_list_df, graph_df, on=['model', 'dataset'], how='inner')
    merged_df = pd.merge(merged_df, graph_macro_df, on=['model', 'dataset'], how='inner')
    
    return merged_df

def print_top_models_idgg(top_models):
    """
    æ‰“å°æ¯ä¸ªæ•°æ®é›†çš„é¡¶çº§æ¨¡å‹ (IDGG ç‰ˆæœ¬)
    """
    print("\n" + "="*80)
    print("IDGG Social Fidelity å„æ•°æ®é›†é¡¶çº§æ¨¡å‹åˆ†æç»“æœ")
    print("="*80)
    
    for dataset, models in top_models.items():
        print(f"\næ•°æ®é›†: {dataset}")
        print("-" * 50)
        print(f"  æœ€é«˜ Macro Structure Score æ¨¡å‹: {models['macro_structure']['model']} (å¾—åˆ†: {models['macro_structure']['score']:.4f})")
        print(f"  æœ€é«˜ Macro Phenomenon Score æ¨¡å‹: {models['macro_phenomenon']['model']} (å¾—åˆ†: {models['macro_phenomenon']['score']:.4f})")
        print(f"  æœ€é«˜ Fidelity Score æ¨¡å‹: {models['fidelity']['model']} (å¾—åˆ†: {models['fidelity']['score']:.4f})")

def find_top_models_per_dataset_idgg(df):
    """
    æ‰¾å‡ºæ¯ä¸ªæ•°æ®é›†ä¸­ä¸‰ä¸ª IDGG æŒ‡æ ‡çš„æœ€é«˜åˆ†æ¨¡å‹
    """
    top_models = {}
    
    # æŒ‰æ•°æ®é›†åˆ†ç»„
    for dataset in df['dataset'].drop_duplicates().values:
        dataset_df = df[df['dataset'] == dataset]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if dataset_df.empty:
            continue
            
        # æ‰¾åˆ°æ¯ä¸ªæŒ‡æ ‡çš„æœ€é«˜åˆ†æ¨¡å‹
        top_structure = dataset_df.loc[dataset_df['macro_structure_score'].idxmax()]
        top_phenomenon = dataset_df.loc[dataset_df['macro_phenomenon_score'].idxmax()]
        top_fidelity = dataset_df.loc[dataset_df['idgg_social_fidelity_score'].idxmax()]
        
        top_models[dataset] = {
            'dataset': dataset,
            'macro_structure': {
                'model': top_structure['model'],
                'score': top_structure['macro_structure_score']
            },
            'macro_phenomenon': {
                'model': top_phenomenon['model'],
                'score': top_phenomenon['macro_phenomenon_score']
            },
            'fidelity': {
                'model': top_fidelity['model'],
                'score': top_fidelity['idgg_social_fidelity_score']
            }
        }
    
    return top_models

def evaluate_idgg_social_fidelity(
    graph_list_file_path="LLMGGen/reports/concat/merged_graph_list_matrix.csv",
    graph_file_path="LLMGGen/reports/concat/merged_graph_matrix.csv",
    graph_macro_file_path="LLMGGen/reports/concat/merged_graph_macro_matrix.csv",
    output_file_path="LLMGGen/reports/idgg_social_fidelity_scores.csv",
    exclude_models=None,
    weights=None
):
    """
    ä¸»å‡½æ•°ï¼šè¯„ä¼° IDGG social fidelity
    """
    # åŠ è½½å’Œå¤„ç†æ•°æ®ï¼Œæ’é™¤æŒ‡å®šæ¨¡å‹
    graph_list_df = load_and_process_graph_list_data(graph_list_file_path, exclude_models)
    graph_df = load_and_process_graph_data(graph_file_path, exclude_models)
    graph_macro_df = load_and_process_graph_macro_data(graph_macro_file_path, exclude_models)
    
    # åˆå¹¶æ•°æ®ï¼Œåªä¿ç•™æ‰€æœ‰è¡¨éƒ½åŒ…å«çš„ model å’Œ dataset ç»„åˆ
    merged_df = merge_all_datasets(graph_list_df, graph_df, graph_macro_df)
    
    # æ£€æŸ¥åˆå¹¶åçš„æ•°æ®
    print(f"åˆå¹¶åçš„æ•°æ®å½¢çŠ¶: {merged_df.shape}")
    print("åˆå¹¶åçš„åˆ—:", merged_df.columns.tolist())
    
    # å½’ä¸€åŒ–æŒ‡æ ‡ï¼ˆä»…ç”¨äºè®¡ç®—åˆ†æ•°ï¼‰
    normalized_df = normalize_metrics_for_idgg(merged_df)
    
    # æ˜¾ç¤ºå½’ä¸€åŒ–åçš„æ ·æœ¬æ•°æ®
    print("å½’ä¸€åŒ–åçš„æ ·æœ¬æ•°æ®:")
    print(normalized_df.head())
    
    # è®¡ç®— IDGG social fidelity scores
    result_df = calculate_idgg_social_fidelity_scores(merged_df, normalized_df, weights)
    
    # æ˜¾ç¤ºè®¡ç®—åçš„æ ·æœ¬æ•°æ®
    print("è®¡ç®—åˆ†æ•°åçš„æ ·æœ¬æ•°æ®:")
    print(result_df[['model', 'dataset', 'macro_structure_score', 'macro_phenomenon_score', 'idgg_social_fidelity_score']].head())
    
    # ä¿å­˜ç»“æœï¼ˆåŒ…å«åŸå§‹æŒ‡æ ‡å€¼å’Œè®¡ç®—åˆ†æ•°ï¼‰
    output_path = Path(output_file_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜åŒ…å«åŸå§‹æŒ‡æ ‡å’Œè®¡ç®—åˆ†æ•°çš„æ•°æ®
    result_df.to_csv(output_file_path, index=False)
    
    print(f"âœ… IDGG è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_file_path}")
    print(f"ğŸ“Š æ€»å…±è¯„ä¼°äº† {len(result_df)} ä¸ª model-dataset ç»„åˆ")
    print("ğŸ“‹ å‰5è¡Œç»“æœ:")
    print(result_df.head())
    
    # æ‰¾å‡ºæ¯ä¸ªæ•°æ®é›†çš„é¡¶çº§æ¨¡å‹
    top_models = find_top_models_per_dataset_idgg(result_df)
    print_top_models_idgg(top_models)
    
    return result_df

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¯„ä¼° IDGG social fidelity score")
    parser.add_argument("--graph_list_file", type=str, 
                        default="LLMGGen/reports/concat/merged_graph_list_matrix.csv",
                        help="graph list çŸ©é˜µæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--graph_file", type=str,
                        default="LLMGGen/reports/concat/merged_graph_matrix.csv",
                        help="graph çŸ©é˜µæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--graph_macro_file", type=str,
                        default="LLMGGen/reports/concat/merged_graph_macro_matrix.csv",
                        help="graph macro çŸ©é˜µæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_file", type=str,
                        default="LLMGGen/reports/idgg_social_fidelity_scores.csv",
                        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--exclude_models", type=str, nargs='*',
                        help="è¦æ’é™¤çš„æ¨¡å‹åˆ—è¡¨ï¼Œä¾‹å¦‚: --exclude_models idgg_csv_processed_edge")
    parser.add_argument("--macro_structure_weight", type=float, default=0.6,
                        help="å®è§‚ç»“æ„éƒ¨åˆ†çš„æƒé‡ (é»˜è®¤: 0.4)")
    parser.add_argument("--macro_phenomenon_weight", type=float, default=0.4,
                        help="å®è§‚ç°è±¡éƒ¨åˆ†çš„æƒé‡ (é»˜è®¤: 0.6)")
    
    args = parser.parse_args()
    
    # è®¾ç½®æƒé‡
    weights = {
        'macro_structure': args.macro_structure_weight,
        'macro_phenomenon': args.macro_phenomenon_weight
    }
    
    # æ‰§è¡Œè¯„ä¼°
    evaluate_idgg_social_fidelity(
        graph_list_file_path=args.graph_list_file,
        graph_file_path=args.graph_file,
        graph_macro_file_path=args.graph_macro_file,
        output_file_path=args.output_file,
        exclude_models=args.exclude_models,
        weights=weights
    )