# LLMGGen/eval_utils/eval_tdgg.py
import pandas as pd
import numpy as np
from pathlib import Path
import argparse
from sklearn.preprocessing import MinMaxScaler

def load_and_process_retrieval_data(file_path, 
                                    exclude_models=None, 
                                    metrics = ['hit@100', 'recall@100']):
    """
    åŠ è½½å¹¶å¤„ç† retrieval æ•°æ® (merged_dst_retrival_matrix.csv)
    å…ˆæŒ‰ Group åˆ—è¿›è¡Œåˆ†ç»„ï¼Œç„¶åå¯¹åŒ dataset, model, Group çš„æ•°æ®è¿›è¡Œåˆ†ç»„å¹¶è®¡ç®—å¹³å‡å€¼
    """
    df = pd.read_csv(file_path)
    
    # å¦‚æœæä¾›äº†è¦æ’é™¤çš„æ¨¡å‹åˆ—è¡¨ï¼Œåˆ™è¿‡æ»¤æ‰è¿™äº›æ¨¡å‹
    if exclude_models:
        df = df[~df['model'].isin(exclude_models)]
        print(f"ğŸ” ä» retrieval æ•°æ®ä¸­æ’é™¤äº† {len(exclude_models)} ä¸ªæ¨¡å‹: {exclude_models}")
    
    # é€‰æ‹©éœ€è¦çš„æŒ‡æ ‡ (ä½¿ç”¨ hit@100 ç›¸å…³æŒ‡æ ‡)
    
    
    # ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
    required_columns = ['model', 'dataset', 'Group'] + metrics
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"ç¼ºå¤±åˆ—: {missing_columns}")
    
    # æŒ‰ model, dataset, Group åˆ†ç»„å¹¶è®¡ç®—å¹³å‡å€¼
    grouped_df = df.groupby(['model', 'dataset', 'Group'])[metrics].mean().reset_index()
    
    # é€è§†è¡¨è½¬æ¢ï¼Œä½¿ Group æˆä¸ºåˆ—
    pivot_df = grouped_df.pivot_table(index=['model', 'dataset'], 
                                      columns='Group', 
                                      values=metrics,
                                      aggfunc='mean')
    
    # æ‰å¹³åŒ–åˆ—å
    pivot_df.columns = [f'{metric}_{group}' for metric, group in pivot_df.columns]
    
    # é‡ç½®ç´¢å¼•
    pivot_df = pivot_df.reset_index()
    
    # é‡å‘½ååˆ—ä»¥æ ‡è¯†æ¥æº
    rename_dict = {col: f"retrieval_{col}" for col in pivot_df.columns if col not in ['model', 'dataset']}
    pivot_df.rename(columns=rename_dict, inplace=True)
    
    return pivot_df

def load_and_process_edge_data(file_path, exclude_models=None):
    """
    åŠ è½½å¹¶å¤„ç† edge æ•°æ® (merged_edge_matrix.csv)
    é€‰æ‹©æŒ‡å®šæŒ‡æ ‡
    """
    df = pd.read_csv(file_path)
    
    # å¦‚æœæä¾›äº†è¦æ’é™¤çš„æ¨¡å‹åˆ—è¡¨ï¼Œåˆ™è¿‡æ»¤æ‰è¿™äº›æ¨¡å‹
    if exclude_models:
        df = df[~df['model'].isin(exclude_models)]
        print(f"ğŸ” ä» edge æ•°æ®ä¸­æ’é™¤äº† {len(exclude_models)} ä¸ªæ¨¡å‹: {exclude_models}")
    
    # é€‰æ‹©éœ€è¦çš„æŒ‡æ ‡
    metrics = ['label_acc', 'ROUGE_L', 'BERTScore_F1']
    
    # ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
    required_columns = ['model', 'dataset'] + metrics
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"ç¼ºå¤±åˆ—: {missing_columns}")
    
    # æŒ‰ model å’Œ dataset åˆ†ç»„å¹¶è®¡ç®—å¹³å‡å€¼
    grouped_df = df.groupby(['model', 'dataset'])[metrics].mean().reset_index()
    
    # é‡å‘½ååˆ—ä»¥æ ‡è¯†æ¥æº
    rename_dict = {metric: f"edge_{metric}" for metric in metrics}
    grouped_df.rename(columns=rename_dict, inplace=True)
    
    return grouped_df

def merge_datasets(retrieval_df, edge_df):
    """
    åˆå¹¶ä¸¤ä¸ªæ•°æ®é›†ï¼Œåªä¿ç•™ä¸¤ä¸ªè¡¨éƒ½åŒ…å«çš„ model å’Œ dataset ç»„åˆ
    """
    # ä½¿ç”¨ inner join åªä¿ç•™ä¸¤ä¸ªè¡¨éƒ½æœ‰çš„ model-dataset ç»„åˆ
    merged_df = pd.merge(retrieval_df, edge_df, on=['model', 'dataset'], how='inner')
    return merged_df

def normalize_metrics(df):
    """
    å¯¹æŒ‡æ ‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼ˆæŒ‰ dataset åˆ†ç»„è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»…ç”¨äºè®¡ç®—åˆ†æ•°ï¼Œä¸ä¿®æ”¹åŸå§‹æ•°æ®ï¼‰
    """
    # å¤åˆ¶æ•°æ®æ¡†ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    normalized_df = df.copy()
    
    # è·å–æ‰€æœ‰ retrieval æŒ‡æ ‡åˆ—
    retrieval_metrics = [col for col in df.columns if col.startswith('retrieval_') and col not in ['model', 'dataset']]
    
    # è·å–æ‰€æœ‰ edge æŒ‡æ ‡åˆ—
    edge_metrics = [col for col in df.columns if col.startswith('edge_') and col not in ['model', 'dataset']]
    
    # æŒ‰ dataset åˆ†ç»„è¿›è¡Œå½’ä¸€åŒ–
    for dataset in df['dataset'].unique():
        # è·å–å½“å‰ dataset çš„æ•°æ®ç´¢å¼•
        dataset_mask = df['dataset'] == dataset
        dataset_indices = df[dataset_mask].index
        
        # å¯¹ retrieval æŒ‡æ ‡è¿›è¡Œå½’ä¸€åŒ– (è¿™äº›æŒ‡æ ‡è¶Šé«˜è¶Šå¥½)
        if retrieval_metrics:
            scaler = MinMaxScaler()
            normalized_values = scaler.fit_transform(df.loc[dataset_indices, retrieval_metrics].fillna(0))
            normalized_df.loc[dataset_indices, retrieval_metrics] = normalized_values
        
        # å¯¹ edge æŒ‡æ ‡è¿›è¡Œå½’ä¸€åŒ– (è¿™äº›æŒ‡æ ‡è¶Šé«˜è¶Šå¥½)
        if edge_metrics:
            scaler = MinMaxScaler()
            normalized_values = scaler.fit_transform(df.loc[dataset_indices, edge_metrics].fillna(0))
            normalized_df.loc[dataset_indices, edge_metrics] = normalized_values
    
    return normalized_df

def calculate_tdgg_social_fidelity_score(original_df, normalized_df, weights=None):
    """
    è®¡ç®— tdgg-social fidelity score
    é»˜è®¤æƒé‡ä¸º retrieval: 0.5, edge: 0.5
    """
    if weights is None:
        # é»˜è®¤æƒé‡
        weights = {
            'retrieval': 0.5,
            'edge': 0.5
        }
    
    # åˆ›å»ºç»“æœDataFrameï¼ŒåŒ…å«åŸå§‹æ•°æ®
    result_df = original_df.copy()
    
    # è·å–æ‰€æœ‰ retrieval æŒ‡æ ‡åˆ—
    retrieval_metrics = [col for col in normalized_df.columns if col.startswith('retrieval_') and col not in ['model', 'dataset']]
    
    # è·å–æ‰€æœ‰ edge æŒ‡æ ‡åˆ—
    edge_metrics = [col for col in normalized_df.columns if col.startswith('edge_') and col not in ['model', 'dataset']]
    
    # è®¡ç®— retrieval éƒ¨åˆ†çš„å¹³å‡å¾—åˆ†ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„å€¼ï¼‰
    if retrieval_metrics:
        result_df['retrieval_score'] = normalized_df[retrieval_metrics].mean(axis=1)
    else:
        result_df['retrieval_score'] = 0
    
    # è®¡ç®— edge éƒ¨åˆ†çš„å¹³å‡å¾—åˆ†ï¼ˆä½¿ç”¨å½’ä¸€åŒ–åçš„å€¼ï¼‰
    if edge_metrics:
        result_df['edge_score'] = normalized_df[edge_metrics].mean(axis=1)
    else:
        result_df['edge_score'] = 0
    
    # è®¡ç®—æœ€ç»ˆçš„ tdgg-social fidelity score
    result_df['tdgg_social_fidelity_score'] = (
        weights['retrieval'] * result_df['retrieval_score'] + 
        weights['edge'] * result_df['edge_score']
    )
    
    return result_df

def find_top_models_per_dataset(df):
    """
    æ‰¾å‡ºæ¯ä¸ªæ•°æ®é›†ä¸­ retrieval_scoreã€edge_score å’Œ tdgg_social_fidelity_score æœ€é«˜çš„æ¨¡å‹
    """
    top_models = {}
    
    # æŒ‰æ•°æ®é›†åˆ†ç»„
    for dataset in df['dataset'].unique():
        dataset_df = df[df['dataset'] == dataset]
        
        # æ‰¾åˆ°æ¯ä¸ªæŒ‡æ ‡çš„æœ€é«˜åˆ†æ¨¡å‹
        top_retrieval = dataset_df.loc[dataset_df['retrieval_score'].idxmax()]
        top_edge = dataset_df.loc[dataset_df['edge_score'].idxmax()]
        top_fidelity = dataset_df.loc[dataset_df['tdgg_social_fidelity_score'].idxmax()]
        
        top_models[dataset] = {
            'retrieval': {
                'model': top_retrieval['model'],
                'score': top_retrieval['retrieval_score']
            },
            'edge': {
                'model': top_edge['model'],
                'score': top_edge['edge_score']
            },
            'fidelity': {
                'model': top_fidelity['model'],
                'score': top_fidelity['tdgg_social_fidelity_score']
            }
        }
    
    return top_models

def print_top_models(top_models):
    """
    æ‰“å°æ¯ä¸ªæ•°æ®é›†çš„é¡¶çº§æ¨¡å‹
    """
    print("\n" + "="*80)
    print("å„æ•°æ®é›†é¡¶çº§æ¨¡å‹åˆ†æç»“æœ")
    print("="*80)
    
    for dataset, models in top_models.items():
        print(f"\næ•°æ®é›†: {dataset}")
        print("-" * 50)
        print(f"  æœ€é«˜ Retrieval Score æ¨¡å‹: {models['retrieval']['model']} (å¾—åˆ†: {models['retrieval']['score']:.4f})")
        print(f"  æœ€é«˜ Edge Score æ¨¡å‹: {models['edge']['model']} (å¾—åˆ†: {models['edge']['score']:.4f})")
        print(f"  æœ€é«˜ Fidelity Score æ¨¡å‹: {models['fidelity']['model']} (å¾—åˆ†: {models['fidelity']['score']:.4f})")



import re

# å‡è®¾ retrieval_df å’Œ edge_df å·²ç»åŠ è½½

# å¯¹ retrieval_df åº”ç”¨é‡å‘½åè§„åˆ™
def rename_retrieval_model(model_name):
    if re.match(r'grpo_.*_LIKR_reward_query_.*', model_name):
        return 'LLMGGen-seq'
    elif model_name.startswith('grpo_'):
        return 'LLMGGen'
    return model_name



# å¯¹ edge_df åº”ç”¨é‡å‘½åè§„åˆ™
def rename_edge_model(model_name):
    if re.match(r'grpo_.*_sotopia_edge_.*', model_name):
        return 'LLMGGen-seq'
    elif model_name.startswith('grpo_'):
        return 'LLMGGen'
    return model_name



def evaluate_tdgg_social_fidelity(
    retrieval_file_path="LLMGGen/reports/concat/merged_dst_retrival_matrix.csv",
    edge_file_path="LLMGGen/reports/concat/merged_edge_matrix.csv",
    output_file_path="LLMGGen/reports/tdgg_social_fidelity_scores.csv",
    exclude_models=None,
    weights=None
):
    """
    ä¸»å‡½æ•°ï¼šè¯„ä¼° tdgg-social fidelity
    """
    # åŠ è½½å’Œå¤„ç†æ•°æ®ï¼Œæ’é™¤æŒ‡å®šæ¨¡å‹
    retrieval_df = load_and_process_retrieval_data(retrieval_file_path, exclude_models)
    retrieval_df['model'] = retrieval_df['model'].apply(rename_retrieval_model)
    edge_df = load_and_process_edge_data(edge_file_path, exclude_models)
    edge_df['model'] = edge_df['model'].apply(rename_edge_model)
    
    # åˆå¹¶æ•°æ®ï¼Œåªä¿ç•™ä¸¤ä¸ªè¡¨éƒ½åŒ…å«çš„ model å’Œ dataset ç»„åˆ
    merged_df = merge_datasets(retrieval_df, edge_df)
    
    # å½’ä¸€åŒ–æŒ‡æ ‡ï¼ˆä»…ç”¨äºè®¡ç®—åˆ†æ•°ï¼‰
    normalized_df = normalize_metrics(merged_df)
    
    # è®¡ç®— tdgg-social fidelity score
    result_df = calculate_tdgg_social_fidelity_score(merged_df, normalized_df, weights)
    
    # ä¿å­˜ç»“æœï¼ˆåŒ…å«åŸå§‹æŒ‡æ ‡å€¼å’Œè®¡ç®—åˆ†æ•°ï¼‰
    output_path = Path(output_file_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    result_df.to_csv(output_file_path, index=False)
    
    print(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_file_path}")
    print(f"ğŸ“Š æ€»å…±è¯„ä¼°äº† {len(result_df)} ä¸ª model-dataset ç»„åˆ")
    print("ğŸ“‹ å‰5è¡Œç»“æœ:")
    print(result_df.head())
    
    # æ‰¾å‡ºæ¯ä¸ªæ•°æ®é›†çš„é¡¶çº§æ¨¡å‹
    top_models = find_top_models_per_dataset(result_df)
    print_top_models(top_models)
    
    return result_df

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è¯„ä¼° tdgg-social fidelity score")
    parser.add_argument("--retrieval_file", type=str, 
                        default="LLMGGen/reports/concat/merged_dst_retrival_matrix_raw.csv",
                        help="retrieval çŸ©é˜µæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--edge_file", type=str,
                        default="LLMGGen/reports/concat/merged_edge_matrix.csv",
                        help="edge çŸ©é˜µæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_file", type=str,
                        default="LLMGGen/reports/tdgg_social_fidelity_scores.csv",
                        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--exclude_models", type=str, nargs='*',
                        help="è¦æ’é™¤çš„æ¨¡å‹åˆ—è¡¨ï¼Œä¾‹å¦‚: --exclude_models model1 model2")
    parser.add_argument("--retrieval_weight", type=float, default=0.5,
                        help="retrieval éƒ¨åˆ†çš„æƒé‡ (é»˜è®¤: 0.5)")
    parser.add_argument("--edge_weight", type=float, default=0.5,
                        help="edge éƒ¨åˆ†çš„æƒé‡ (é»˜è®¤: 0.5)")
    
    args = parser.parse_args()
    
    # è®¾ç½®æƒé‡
    weights = {
        'retrieval': args.retrieval_weight,
        'edge': args.edge_weight
    }
    
    # æ‰§è¡Œè¯„ä¼°
    evaluate_tdgg_social_fidelity(
        retrieval_file_path=args.retrieval_file,
        edge_file_path=args.edge_file,
        output_file_path=args.output_file,
        exclude_models=args.exclude_models,
        weights=weights
    )